{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6cb8faf",
   "metadata": {},
   "source": [
    "MediaPipe is an open-source framework developed by Google that provides a comprehensive solution for building real-time perception pipelines for a wide range of applications, including object detection, facial recognition, hand tracking, pose estimation, and more. It offers pre-trained machine learning models, as well as tools and components for processing, analyzing, and interpreting various forms of media input, such as images, video streams, and sensor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0df0a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.1\n"
     ]
    }
   ],
   "source": [
    "# Required imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from collections import deque\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "944f7c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the coordinates of the points traversed by the finger \n",
    "bpoints = [deque(maxlen=1024)]\n",
    "gpoints = [deque(maxlen=1024)]\n",
    "rpoints = [deque(maxlen=1024)]\n",
    "ypoints = [deque(maxlen=1024)]\n",
    "\n",
    "\n",
    "# These indexes will be used to mark the points in particular arrays of specific colour\n",
    "blue_index = 0\n",
    "green_index = 0\n",
    "red_index = 0\n",
    "yellow_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "298de55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors in BGR format Blue, Green, Red, Yellow \n",
    "colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (0, 255, 255)]\n",
    "\n",
    "# Which color is selected\n",
    "colorIndex = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6739c3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paint window of 471 length, 636 breadth, 3 width , white color\n",
    "paintWindow = np.zeros((471,636,3)) + 255\n",
    "\n",
    "cv2.namedWindow('Paint', cv2.WINDOW_AUTOSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51b775b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise Mediapipe\n",
    "\n",
    "# This module provides functionality for detecting and tracking hands in images or video streams.\n",
    "mpHands = mp.solutions.hands\n",
    "\n",
    "# max_num_hands=1 specifies that the model should detect at most one hand in the input\n",
    "# min_detection_confidence=0.7 means hand detections with a confidence score >= 0.7 will be considered valid.\n",
    "hands = mpHands.Hands(max_num_hands = 1, min_detection_confidence = 0.7)\n",
    "\n",
    "# provides utility functions for drawing landmarks and connections on images or video frames\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "# model = load_model('/Air_Canvas/mp_hand_gesture')\n",
    "\n",
    "\n",
    "labels = ['okay', 'peace', 'thumbs up', 'thumbs down', 'call me', 'stop', 'rock', 'live long', 'fist', 'smile']\n",
    "flag = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9777b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "ret = True\n",
    "\n",
    "while ret:\n",
    "    # Used to capture a frame from a video\n",
    "    # read() returns tow values boolean ret = True when the frame is captured successfully\n",
    "    # frame contains the actual image data of the captured frame.\n",
    "    ret, frame = cap.read()\n",
    "    # x = heaight, y = width, c = no.of channels(1 if img is in gray scale, 3 if in RGB)\n",
    "    x, y, c = frame.shape\n",
    "    \n",
    "    \n",
    "    # Flip the along vertical axis (1 - along vertical axis, 0 means along horizontal axis)\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    # converting img from bgr format to rgb format\n",
    "    framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Used to draw a rectangle on an img Frame, top left coordinates, bottom right coordinates, color of rectangle, border width in pixels   \n",
    "    frame = cv2.rectangle(frame, (20, 15), (96, 48), (0, 0, 0), 2)\n",
    "    frame = cv2.rectangle(frame, (118, 15), (194, 48), (255, 0, 0), 2)\n",
    "    frame = cv2.rectangle(frame, (216, 15), (292, 48), (0, 255, 0), 2)\n",
    "    frame = cv2.rectangle(frame, (314, 15), (390, 48), (0, 0, 255), 2)\n",
    "    frame = cv2.rectangle(frame, (412, 15), (488, 48), (0, 255, 255), 2)\n",
    "    frame = cv2.rectangle(frame, (510, 15), (586, 48), (226, 43, 138), 2)\n",
    "    frame = cv2.rectangle(frame, (612, 15), (636, 48), (255, 255, 0), 2)\n",
    "\n",
    "    cv2.putText(frame, \"CLEAR\", (30, 33), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "    cv2.putText(frame, \"BLUE\", (133, 33), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "    cv2.putText(frame, \"GREEN\", (225, 33), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "    cv2.putText(frame, \"RED\", (335, 33), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "    cv2.putText(frame, \"YELLOW\", (420, 33), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "    cv2.putText(frame, \"LM\", (535, 33), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "     if flag == 1:\n",
    "        if result.multi_hand_landmarks:\n",
    "            mpDraw.draw_landmarks(frame, result.multi_hand_landmarks[0], mpHands.HAND_CONNECTIONS)\n",
    "    # Get hand landmark prediction\n",
    "    # result contains information such as the detected hand landmarks (e.g. keypoints representing fingertips, palm center)\n",
    "    result = hands.process(framergb)\n",
    "    \n",
    "    className = ''\n",
    "    \n",
    "    # if the result conatains hand\n",
    "    hand_present = result.multi_hand_landmarks\n",
    "    if result.multi_hand_landmarks:\n",
    "        landmarks = []\n",
    "        \n",
    "        for lm in result.multi_hand_landmarks[0].landmark:\n",
    "\n",
    "                # scaling the coordinates of landmarks acc to the paintWindow because the values are in [0,1]\n",
    "                lmx = int(lm.x * 640)\n",
    "                lmy = int(lm.y * 480)\n",
    "\n",
    "                landmarks.append([lmx, lmy])\n",
    "                \n",
    "        \n",
    "         # Predict gesture in Hand Gesture Recognition project\n",
    "         # prediction = model.predict([landmarks]) \n",
    "\n",
    "         # classID = np.argmax(prediction)\n",
    "         # className = classNames[classID]\n",
    "\n",
    "        cv2.putText(frame, className, (10, 50), cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        fore_finger = (landmarks[8][0],landmarks[8][1])\n",
    "        thumb_finger = (landmarks[4][0],landmarks[4][1])\n",
    "        \n",
    "        distance = math.sqrt((fore_finger[0] - thumb_finger[0])**2 + (fore_finger[1] - thumb_finger[1])**2)         \n",
    "        # do not write on the paintWindow when fore_finger and thumb are closer\n",
    "        if distance < 30:\n",
    "            bpoints.append(deque(maxlen=512))\n",
    "            blue_index += 1\n",
    "            gpoints.append(deque(maxlen=512))\n",
    "            green_index += 1\n",
    "            rpoints.append(deque(maxlen=512))\n",
    "            red_index += 1\n",
    "            ypoints.append(deque(maxlen=512))\n",
    "            yellow_index += 1\n",
    "           \n",
    "        # when the finger is in the button region\n",
    "        elif fore_finger[1] <= 65:\n",
    "            \n",
    "            # Clear button\n",
    "            if 40 <= fore_finger[0] <= 145:\n",
    "                bpoints = [deque(maxlen=512)]\n",
    "                gpoints = [deque(maxlen=512)]\n",
    "                rpoints = [deque(maxlen=512)]\n",
    "                ypoints = [deque(maxlen=512)]\n",
    "\n",
    "                blue_index = 0\n",
    "                green_index = 0\n",
    "                red_index = 0\n",
    "                yellow_index = 0\n",
    "\n",
    "                # clearing the paint window\n",
    "                paintWindow[66:,:,:] = 255\n",
    "                \n",
    "            elif 160 <= fore_finger[0] <= 255 : # Blue button\n",
    "                    colorIndex = 0 \n",
    "            elif 275 <= fore_finger[0] <= 370 : # Green button\n",
    "                    colorIndex = 1\n",
    "            elif 390 <= fore_finger[0] <= 485 : # Red button\n",
    "                    colorIndex = 2 \n",
    "            elif 505 <= fore_finger[0] <= 600 : # Yellow button\n",
    "                    colorIndex = 3  \n",
    "            elif 615 <= fore_finger[0] <= 635 : # Yellow button\n",
    "                    if flag == 0:\n",
    "                        flag = 1\n",
    "                    elif flag == 1:\n",
    "                        flag = 0\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "        else :\n",
    "            if colorIndex == 0:\n",
    "                bpoints[blue_index].appendleft(fore_finger)\n",
    "            elif colorIndex == 1:\n",
    "                gpoints[green_index].appendleft(fore_finger)\n",
    "            elif colorIndex == 2:\n",
    "                rpoints[red_index].appendleft(fore_finger)\n",
    "            elif colorIndex == 3:\n",
    "                ypoints[yellow_index].appendleft(fore_finger)\n",
    "    else :\n",
    "\n",
    "        bpoints.append(deque(maxlen=512))\n",
    "        blue_index += 1\n",
    "        gpoints.append(deque(maxlen=512))\n",
    "        green_index += 1\n",
    "        rpoints.append(deque(maxlen=512))\n",
    "        red_index += 1\n",
    "        ypoints.append(deque(maxlen=512))\n",
    "        yellow_index += 1\n",
    "            \n",
    "    points = [bpoints, gpoints, rpoints, ypoints]\n",
    "\n",
    "    for i in range(len(points)):\n",
    "        for j in range(len(points[i])):\n",
    "            for k in range(1, len(points[i][j])):\n",
    "                if points[i][j][k - 1] is None or points[i][j][k] is None:\n",
    "                    continue\n",
    "                cv2.line(frame, points[i][j][k - 1], points[i][j][k], colors[i], 2)\n",
    "                cv2.line(paintWindow, points[i][j][k - 1], points[i][j][k], colors[i], 2)\n",
    "\n",
    "    cv2.imshow(\"Output\", frame) \n",
    "    cv2.imshow(\"Paint\", paintWindow)\n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# release the webcam and destroy all active windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e0f257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf153692",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
